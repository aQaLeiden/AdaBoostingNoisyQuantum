# -*- coding: utf-8 -*-
"""Noisy_boosted

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n_KXtZsINafeKtJnnjoj1dJZ6stkRW1k
"""



import tensorflow as tf
import tensorflow_quantum as tfq
import cirq
import sympy
import numpy as np
import pandas as pd
# %matplotlib inline
import matplotlib.pyplot as plt
from cirq.contrib.svg import SVGCircuit
import random
import sys
import math
import time
from sklearn.model_selection import StratifiedKFold
from sklearn.datasets import make_moons
import matplotlib
from sklearn.preprocessing import MinMaxScaler

np.set_printoptions(threshold=sys.maxsize)
epochs=10
number_of_models=10
attributes=2
batch_size=32
shots=100
number_of_folds=5
repetitions=1
noise_probability=0.1

"""#Data preparation"""

input_data, y_labels=make_moons(n_samples=100, shuffle=True, noise=None, random_state=0)

fig = plt.figure()
colors = ['red','blue']
scattered=plt.scatter(input_data[:,0],input_data[:,1], c=(2*y_labels-1), cmap=matplotlib.colors.ListedColormap(colors))
plt.legend(*scattered.legend_elements())
plt.title('Dataset')
plt.xlabel('x')
plt.ylabel('y')
plt.show()
print(input_data[0])

#make a -1, +1 array of ydata
y_data=np.asarray(y_labels).astype('float32')
y_data=tf.reshape(y_data, (len(y_data), 1))
y_data=2*y_data-1

#check whether the datasets are divided reasonably
neg, pos = np.bincount(y_labels)
total = neg + pos
print('Examples in total set:\n    Total: {}\n    Positive: {} ({:.2f}% of total)\n'.format(
    total, pos, 100 * pos / total))

"""##Functions"""

#normalize all the values with respect to the training datas means
def Normalization(training_data, test_data):
  '''normalize the data wrt the training data mean within a range from -pi to pi'''
  norm = MinMaxScaler(feature_range=(-np.pi, np.pi))
  norm.fit(training_data)
  train_norm = norm.transform(training_data)
  test_norm = norm.transform(test_data)
  return train_norm, test_norm

#now we build an encoding circuit for encoding the data
def circuit_transform(datapoint, noisy):
  '''transform a datapoint with ... attributes to a circuit'''
  qubits=[cirq.GridQubit(0,i) for i in range(0, attributes)]
  circuit=cirq.Circuit()
  for i, angle in enumerate(datapoint):
    circuit.append(cirq.ry(angle).on(qubits[i]))
    if noisy==True:
      circuit.append(cirq.depolarize(noise_probability).on(qubits[i]))
  return circuit

def rounding_array(results):
  '''Assign the values -1 and 1 to the predictions, depending on whether the prediciton is above or below 0'''
  results2=np.asarray(results).copy()
  for i in range (0, len(results2)):
    if results2[i] >= 0.0:
      results2[i]= 1.0
    elif results2[i] <0.0:
      results2[i] =-1.0
  return np.squeeze(results2)

def hinge_accuracy(y_true, y_pred):
  y_true = tf.squeeze(y_true) > 0.0
  y_pred = tf.squeeze(y_pred) > 0.0
  result = tf.cast(y_true == y_pred, tf.float32)

  return tf.reduce_mean(result)

def QNN(gate, qubits, readout, layernumber, noisy):
  '''Construct a layer of aQuantum neural network with controlled gates to the readout qubit, input the kind of entangling gate, the qubits, the readout qubit
  and the the layer of the QNN'''
  circuit=cirq.Circuit()
  for i, qubit in enumerate(qubits):
    symbol=sympy.Symbol('Theta'+str(i)+'_'+str(layernumber))
    circuit.append(gate(qubit, readout)**symbol) 
    if noisy==True:
      circuit.append(cirq.depolarize(noise_probability).on(qubit))
      circuit.append(cirq.depolarize(noise_probability).on(readout))
  return circuit

def model_creation(noisy):
  '''Construct full circuit'''
  qubits=[cirq.GridQubit(0,i) for i in range(0, attributes)]
  readout_qubit=cirq.GridQubit(-1,-1)
  circuit=cirq.Circuit()

  circuit.append(cirq.H(readout_qubit))
  if noisy==True:
    circuit.append(cirq.depolarize(noise_probability).on(readout_qubit))

  circuit.append(QNN(cirq.XX, qubits, readout_qubit,0, noisy))
  circuit.append(QNN(cirq.ZZ, qubits, readout_qubit,1, noisy))
  circuit.append(QNN(cirq.YY, qubits, readout_qubit,2, noisy))

  circuit.append(cirq.H(readout_qubit))
  if noisy==True:
    circuit.append(cirq.depolarize(noise_probability).on(readout_qubit))

  readout_measurement=cirq.Z(readout_qubit)
  return circuit, readout_measurement

noisy_model, noisy_readout=model_creation(noisy=True)
SVGCircuit(noisy_model)

def Boosting(training_set, training_labels, number_of_models, test_data, test_labels, shots, noisy):
  '''This function performs the boosting algorithm
  training_set: set with which all the classifiers are trained
  number_of_models: layers in the boosting algorithm, how many times the algorithm is repeated
  test_data: set of test_data to make predictions on
  test_labels:labels corresponding to the test data

  outputs:
  predictions on the test data. If the algorithm stops prematurely, a message is outputted'''

  # create emtpy arrays, for all the hypotheses of each individual model (final_hypothesis),
  # for the distribution over the data (starts as 1/N for eacht datapoint) and
  # the weights, which contains the weights for each hypothesis from each indivdual model
  final_hypothesis = np.zeros((number_of_models, len(test_data)))
  training_hypothesis = np.zeros((number_of_models, len(training_set)))
  distribution = np.ones(len(training_set)) / len((training_set))
  weights = []
  for model in range(0, number_of_models):
    noiseless_model, noiseless_readout = model_creation(noisy) #even though this is called noiseless, this is noisy when noisy=true

    if noisy == True:
      # Build sequential model, with input, PQC
      PQC_layer = tfq.layers.NoisyPQC(noiseless_model, noiseless_readout, sample_based=True, repetitions=shots)
      input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string)
      learner = tf.keras.Sequential([input_layer, PQC_layer])
    if noisy == False:
      PQC_layer = tfq.layers.PQC(noiseless_model, noiseless_readout)
      input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string)
      learner = tf.keras.Sequential([input_layer, PQC_layer])

    # compiling the learner
    learner.compile(
      loss=tf.keras.losses.Hinge(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=[hinge_accuracy])

    # print the learning progress, while fitting to the training data according to the distribution
    print('------------------------------------------------------------------------')
    print('Model', model)
    history = learner.fit(tf.convert_to_tensor(training_set), training_labels,
                          batch_size=32,
                          epochs=epochs,
                          verbose=0,
                          sample_weight=distribution,
                          validation_data=(test_data, test_labels))

    # predictions on training set
    prediction_on_training_set = rounding_array(np.asarray(tf.squeeze(learner.predict(training_set))))

    # calculate error on training set
    error = calculate_error(prediction_on_training_set, training_labels, distribution)

    # predictions on test set
    predictions_rough = tf.squeeze(learner.predict(test_data))
    single_model_prediction = rounding_array((predictions_rough))  # prediction of the individual model

    # if the error is bigger than 0.5, flip hypotheses
    if error >= 0.5:
      print('error')
      error = 1 - error
      single_model_prediction = -single_model_prediction
      prediction_on_training_set = -prediction_on_training_set
      predictions_rough = -predictions_rough

    print(error)

    if model == 0:
      first_predictions = single_model_prediction
      first_training_prediction = prediction_on_training_set

    final_hypothesis[model] = single_model_prediction
    training_hypothesis[model] = prediction_on_training_set

    # calculate the weight of the hypothesis of this model
    weight = calculate_weights(error)
    weights.append(weight)

    # update the distribution, depending on which samples were misclassified
    distribution = update_distribution(prediction_on_training_set, weight, distribution, training_labels)

    # the final prediction is the weighted average of all the hypothesis
  return final_hypothesis, first_predictions, weights, training_hypothesis, first_training_prediction

def calculate_error(predictions, true_labels, distribution):
  '''calculate the weighted error, according to the distribution'''
  error=0
  for i in range(0, len(true_labels)):
    if predictions[i]!=true_labels[i]:
      error+=distribution[i]
  return error

def update_distribution(predictions, weight, distribution, true_labels):
  '''update the distribution, if the sample was misclassified, assign extra weight to its original indice'''
  for i in range(0, len(true_labels)):
    if predictions[i]==true_labels[i]:
      distribution[i]*=math.exp(-weight)
    else:
      distribution[i]*=math.exp(weight)
  distribution=distribution/np.sum(distribution)
  return distribution

def calculate_weights(error):
  '''Ã‡alculate weight of the hypothesis, given its error'''
  if error==0:
    return 10
  return 0.5*math.log((1-error)/error)

"""##Program"""

#kfolds
train_folds=np.ones((number_of_folds, int(len(input_data)-len(input_data)/number_of_folds)), int)
test_folds=np.ones((number_of_folds, int(len(input_data)/number_of_folds)), int)
fold_count=0
kfold = StratifiedKFold(n_splits=number_of_folds, shuffle=True, random_state=0)
for train, test in kfold.split(input_data, y_data):
  train_folds[fold_count]=train
  test_folds[fold_count]=test
  fold_count+=1

#make arrays to store plots in
full_training_graph=np.ones((number_of_folds*repetitions, number_of_models))
full_test_graph=np.ones((number_of_folds*repetitions, number_of_models))

##Boosting without noise
for rep in range(0, repetitions):
  start = time.time()
  train_graph=np.ones((number_of_folds,number_of_models))
  test_graph=np.ones((number_of_folds,number_of_models))

  for fold in range(0, number_of_folds):
    train=train_folds[fold]
    test=test_folds[fold]

  #make test and training data, normalize
    train_data=input_data[train]
    test_data=input_data[test]
    train_data, test_data=Normalization(train_data, test_data)

  #convert data to circuits
    x_data_circuit= [circuit_transform(i, noisy=True) for i in train_data]
    x_train_final=tfq.convert_to_tensor(x_data_circuit)
    x_data_circuit_test= [circuit_transform(i, noisy=True) for i in test_data]
    x_test_final=tfq.convert_to_tensor(x_data_circuit_test)
    y_train=np.asarray(tf.squeeze(y_data))[train].astype(np.float32)
    y_test=np.asarray(tf.squeeze(y_data))[test].astype(np.float32)

    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold} ...')
  
  #boosting
    final_hypothesis, first_predictions, weights, training_hypothesis, first_training_hypothesis \
    =Boosting(x_train_final, y_train, number_of_models, x_test_final, np.asarray(tf.squeeze(y_test)), shots, noisy=True)

  #look at the how the training error progresses as a function of the adaboost models
    wrong_test_array=np.zeros(number_of_models)
    wrong_training_array=np.zeros(number_of_models)
    for i in range(0, number_of_models):
      if i==0:
        training_boosted_predictions=first_training_hypothesis
      else:
        training_boosted_predictions=rounding_array(np.average(training_hypothesis[:i+1,:],axis=0, weights=weights[:i+1]))
      wrong=np.sum(training_boosted_predictions!=np.asarray(tf.squeeze(y_train)))
      wrongly_training_predicted=wrong/len(training_boosted_predictions)
      wrong_training_array[i]=wrongly_training_predicted

  #look at the how the test error progresses as a function of the adaboost models
    for i in range(0, number_of_models):
      if i==0:
        boosted_predictions=first_predictions
      else:
        boosted_predictions=rounding_array(np.average(final_hypothesis[:i+1,:],axis=0, weights=weights[:i+1]))
      wrong=0
      wrong=np.sum(boosted_predictions!=np.asarray(tf.squeeze(y_test)))
      wrongly_predicted=wrong/len(boosted_predictions)
      wrong_test_array[i]=wrongly_predicted

    print('graph')
    print('train', repr(wrong_training_array))
    print('test', repr(wrong_test_array))

    train_graph[fold]=wrong_training_array
    test_graph[fold]=wrong_test_array


  end=time.time()
  print("The time of execution of above program is :", end-start)

  full_training_graph[(number_of_folds*rep):(number_of_folds*(rep+1))]= train_graph
  full_test_graph[(number_of_folds*rep):(number_of_folds*(rep+1))] = test_graph

print('training noisy', repr(full_training_graph))
print('test noisy', repr(full_test_graph))